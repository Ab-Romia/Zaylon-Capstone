# Flowinit - E-commerce AI Agent System

## Project Overview

**Flowinit** is a production-grade, hierarchical multi-agent system for e-commerce customer service automation, built for Instagram/WhatsApp DM interactions. It uses LangGraph for orchestration and OpenAI GPT-4o for intelligence.

### Architecture Type
- **Monolithic FastAPI Application** with internal agent orchestration
- **NOT Microservices** - all agents run in-process
- **Database**: PostgreSQL (async with SQLAlchemy)
- **Vector DB**: Qdrant (for RAG and semantic search)
- **Framework**: LangGraph for agent graph, LangChain for tool binding

---

## System Architecture

### Multi-Agent Hierarchy

```
┌─────────────────────────────────────────────────┐
│              User Request (DM)                  │
└─────────────────┬───────────────────────────────┘
                  │
        ┌─────────▼──────────┐
        │  Load Memory Node  │  (Retrieve customer facts)
        └─────────┬──────────┘
                  │
        ┌─────────▼──────────┐
        │  Supervisor Agent  │  (Route to specialist)
        └─────────┬──────────┘
                  │
        ┌─────────┴──────────┐
        │                    │
  ┌─────▼─────┐      ┌──────▼──────┐
  │  Sales    │      │  Support    │
  │  Agent    │      │  Agent      │
  └─────┬─────┘      └──────┬──────┘
        │                    │
        └─────────┬──────────┘
                  │
        ┌─────────▼──────────┐
        │  Save Memory Node  │  (Extract & save new facts)
        └─────────┬──────────┘
                  │
        ┌─────────▼──────────┐
        │    End State       │
        └────────────────────┘
```

### Agent Responsibilities

**Supervisor Agent:**
- Routes requests to Sales or Support
- Uses GPT-4o-mini for cost efficiency
- Simple text-based routing decision

**Sales Agent:**
- Handles product inquiries, purchases, orders
- Tools: search_products, check_availability, create_order, order_history, order_status
- Uses GPT-4o for tool calling reliability

**Support Agent:**
- Handles FAQs, policies, returns, tracking
- Tools: search_knowledge_base, semantic_product_search, order_history, order_status
- Uses GPT-4o for tool calling reliability

**Memory Nodes:**
- Load: Retrieve customer facts from Memory Bank
- Save: Extract and persist new facts from conversation

---

## Critical Issues & Debugging

### ⚠️ ONGOING ISSUE: Tool Calling Reliability

**Problem**: LLMs inconsistently call tools despite strong prompts.

**Symptoms:**
```
[SALES AGENT] Tools called on first attempt: False
[SALES AGENT] No tools called - forcing retry
```

**Root Causes Identified:**
1. **Old Dependencies**: `langchain-openai==0.0.5` (Jan 2024) - outdated
2. **LLM Decision Making**: Even GPT-4o sometimes chooses not to call tools
3. **API Constraints**: `tool_choice` parameter not well supported in old versions

**Attempted Fixes:**
- ❌ `tool_choice="auto"` - LLM chose not to call tools
- ❌ `tool_choice="required"` - API error: "must provide exactly one tool"
- ❌ `tool_choice="any"` - Same as required, single tool only
- ✅ Retry pattern - Currently implemented but still unreliable

**Current Implementation (app/agents/nodes.py):**
```python
# First attempt with strong system prompt
response = await sales_agent.ainvoke(agent_messages)

# Check if tools were called
tools_called = (
    hasattr(response, 'tool_calls') and
    response.tool_calls is not None and
    isinstance(response.tool_calls, list) and
    len(response.tool_calls) > 0
)

# Retry with forcing message if no tools
if not tools_called:
    agent_messages.append(HumanMessage(
        "STOP. You MUST call a tool before responding..."
    ))
    response = await sales_agent.ainvoke(agent_messages)
```

**Success Rate**: ~40-50% (Target: >80%)

### Recommended Solutions (Not Yet Implemented)

**Option 1: Upgrade Dependencies** ⭐ RECOMMENDED
```bash
pip install --upgrade langchain-openai  # Latest version
pip install --upgrade langchain langchain-community langgraph
```
Newer versions have better `tool_choice` support.

**Option 2: Use LangChain AgentExecutor**
Replace manual tool binding with `create_tool_calling_agent()`:
```python
from langchain.agents import create_tool_calling_agent, AgentExecutor

agent = create_tool_calling_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)
```
This handles tool calling automatically.

**Option 3: Implement Function Calling Fallback**
If LLM doesn't call tools, parse intent from response and manually invoke appropriate tool.

---

## Key Files & Structure

### Core Application
```
main.py                          # FastAPI entry point, startup logic
config.py                        # Settings (Pydantic BaseSettings)
database.py                      # PostgreSQL connection, models
models.py                        # Pydantic schemas
auth.py                          # API key validation, rate limiting
```

### Agent System
```
app/agents/
├── graph.py                     # LangGraph definition, agent orchestration
├── nodes.py                     # Agent node implementations (CRITICAL FILE)
├── state.py                     # FlowinitState schema
└── __init__.py

Key Functions in nodes.py:
- load_memory_node()             # Retrieve customer facts
- supervisor_node()              # Route to sales/support
- sales_agent_node()             # Handle sales queries + tools
- support_agent_node()           # Handle support queries + tools
- save_memory_node()             # Extract & save facts
```

### Tools (LangChain Compatible)
```
app/tools/
├── products_tools.py            # search_products, check_availability
├── orders_tools.py              # create_order, get_order_history, check_order_status
├── rag_tools.py                 # search_knowledge_base, semantic_product_search
├── memory_tools.py              # get_customer_facts, save_customer_fact
└── __init__.py                  # SALES_TOOLS, SUPPORT_TOOLS collections
```

### Services (Business Logic)
```
services/
├── products.py                  # Product search, filtering
├── orders.py                    # Order CRUD operations
├── context.py                   # Conversation context management
├── intent.py                    # Intent classification
├── rag.py                       # RAG retrieval logic
├── vector_db.py                 # Qdrant client wrapper
├── embeddings.py                # OpenAI embeddings service
├── ingestion.py                 # Document ingestion pipeline
├── cache.py                     # Redis-like caching
└── analytics.py                 # Event logging
```

### API Routes
```
routes/
├── agent.py                     # /api/v2/agent/invoke (LangGraph agent)
├── products.py                  # Product endpoints
├── context.py                   # Context management
├── intent.py                    # Intent detection
├── rag.py                       # RAG query endpoints
├── health.py                    # Health checks
└── analytics.py                 # Analytics endpoints
```

### Evaluation & Testing
```
tests/evaluation/
├── golden_dataset.csv           # 30 test cases (Arabic/English/Franco)
├── run_eval.py                  # LLM-as-a-judge evaluation runner
└── EVALUATION_REPORT.md         # Latest evaluation results
```

---

## Database Schema

### Products Table
```sql
CREATE TABLE products (
    id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    price DECIMAL(10,2),
    sizes TEXT[],              -- Array of available sizes
    colors TEXT[],             -- Array of available colors
    stock_count INTEGER,
    category VARCHAR(100),
    is_active BOOLEAN DEFAULT TRUE
);
```

### Orders Table
```sql
CREATE TABLE orders (
    id UUID PRIMARY KEY,
    customer_id VARCHAR(255),
    product_id UUID REFERENCES products(id),
    quantity INTEGER,
    size VARCHAR(50),
    color VARCHAR(50),
    total_amount DECIMAL(10,2),
    status VARCHAR(50),        -- pending, confirmed, shipped, delivered
    delivery_info JSONB,       -- {name, phone, address}
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

### Customer Facts (Memory Bank)
```sql
CREATE TABLE customer_facts (
    id UUID PRIMARY KEY,
    customer_id VARCHAR(255),
    fact_type VARCHAR(50),     -- preference, constraint, personal_info
    fact_key VARCHAR(100),     -- e.g., "preferred_size", "budget_max"
    fact_value TEXT,
    confidence INTEGER,        -- 0-100
    source VARCHAR(50),        -- explicit, inferred
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    UNIQUE(customer_id, fact_key)
);
```

### Context Sessions
```sql
CREATE TABLE context_sessions (
    id UUID PRIMARY KEY,
    customer_id VARCHAR(255),
    channel VARCHAR(50),       -- instagram, whatsapp
    messages JSONB,            -- Array of message objects
    created_at TIMESTAMP,
    last_activity TIMESTAMP
);
```

---

## Environment Variables

Required in `.env`:
```bash
# Database
DATABASE_URL=postgresql+psycopg://user:password@localhost:5432/ecommerce_dm

# API Security
API_KEY=your-secret-api-key

# OpenAI
OPENAI_API_KEY=sk-...

# Qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=optional

# Embeddings
EMBEDDING_MODEL=text-embedding-3-small
USE_LOCAL_EMBEDDINGS=False
EMBEDDING_DIMENSION=1536

# Performance
MAX_CONVERSATION_HISTORY=50
MAX_PRODUCT_SEARCH_RESULTS=5
```

---

## Setup & Deployment

### Local Development
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set up database
createdb ecommerce_dm
psql ecommerce_dm < schema.sql  # If you have schema file

# 3. Set up Qdrant (Docker)
docker run -p 6333:6333 qdrant/qdrant

# 4. Create .env file
cp .env.example .env
# Edit .env with your keys

# 5. Run server
python main.py

# Server runs on http://localhost:8000
```

### Production Deployment
```bash
# Using Gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000

# Docker
docker build -t flowinit .
docker run -p 8000:8000 --env-file .env flowinit
```

---

## API Usage Examples

### Invoke Agent
```bash
curl -X POST http://localhost:8000/api/v2/agent/invoke \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "customer_id": "instagram:@username",
    "message": "I want to buy a red hoodie in size L",
    "channel": "instagram"
  }'
```

**Response:**
```json
{
  "response": "I found a Red Classic Hoodie in size L for 599 EGP...",
  "agent_used": "sales",
  "tool_calls": [
    {
      "tool_name": "search_products_tool",
      "arguments": {"query": "red hoodie", "limit": 5},
      "result": "...",
      "success": true
    }
  ],
  "execution_time_ms": 2341,
  "chain_of_thought": [
    "Loaded customer memory: 3 facts retrieved",
    "Routing decision: SALES (reason: I want to buy...)",
    "Sales agent processed request (used 1 tools)"
  ]
}
```

### Search Products
```bash
curl -X POST http://localhost:8000/api/v1/products/search \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "جينز أزرق",
    "limit": 5
  }'
```

### Create Order
```bash
curl -X POST http://localhost:8000/api/v1/orders \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "customer_id": "instagram:@username",
    "product_id": "uuid-here",
    "quantity": 1,
    "size": "L",
    "color": "blue",
    "delivery_info": {
      "name": "Ahmed Hassan",
      "phone": "+201234567890",
      "address": "123 Tahrir Street, Cairo"
    }
  }'
```

---

## Evaluation System

### Golden Dataset
Located: `tests/evaluation/golden_dataset.csv`

**30 Test Cases:**
- 6 Easy (simple product queries)
- 15 Medium (multilingual, context-dependent)
- 9 Hard (mixed intent, memory recall)

**Languages:**
- 16 English
- 8 Arabic (عربي)
- 6 Franco-Arabic (3ayez, azra2, etc.)

### Running Evaluation
```bash
# Set OpenAI API key
export OPENAI_API_KEY=sk-...

# Run evaluation
cd tests/evaluation
python run_eval.py

# Results saved to EVALUATION_REPORT.md
```

### LLM-as-a-Judge Criteria
1. **Intent Accuracy** (0-1): Correct agent routing?
2. **Tool Selection** (0-1): Appropriate tools called?
3. **Response Quality** (0-1): Helpful, polite, correct language?
4. **Overall Success** (0-1): Would satisfy real customer?

**Target**: >80% overall success rate
**Current**: ~40-50% (tool calling issues)

---

## Performance Characteristics

### Latency (Current)
- **Supervisor Routing**: 800-1500ms
- **Sales Agent (with tools)**: 3000-5000ms
- **Support Agent (with tools)**: 2500-4500ms
- **Total Request**: 5000-8000ms average

### Cost Estimates
- **Supervisor**: ~500 tokens/request × $0.15/1M = $0.000075
- **Sales/Support**: ~2000 tokens/request × $5/1M = $0.01
- **Embeddings**: ~100 tokens × $0.13/1M = $0.000013
- **Total**: ~$0.01-0.015 per conversation turn

### Optimization Opportunities
1. Cache embeddings for common queries
2. Reduce system prompt verbosity
3. Use streaming responses
4. Implement request coalescing
5. Optimize database queries (add indexes)

---

## Debugging Tips

### Enable Debug Logging
```python
# In main.py
logging.basicConfig(
    level=logging.DEBUG,  # Change from INFO
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### Check Tool Execution
Look for these log patterns:
```
INFO - [SALES AGENT] Tools called on first attempt: True/False
INFO - [SALES AGENT] Executing tool: search_products_tool with args: {...}
INFO - [SALES AGENT] Tool search_products_tool succeeded: {...}
```

### Common Issues

**Tools Not Called:**
- Check system prompt in `app/agents/nodes.py`
- Verify `llm_base` is using GPT-4o (not mini)
- Check if LLM response has `tool_calls` attribute

**Database Errors:**
- Ensure PostgreSQL is running
- Check connection string in `.env`
- Verify tables exist: `psql -d ecommerce_dm -c "\dt"`

**Qdrant Warnings:**
- Harmless Pydantic validation warnings
- Already suppressed in code
- Don't affect functionality

**Rate Limiting:**
- Default: 100 requests/minute per API key
- Increase in `config.py`: `rate_limit_per_minute`

---

## Dependencies & Versions

**Critical Dependencies:**
```
fastapi==0.104.1
langchain==0.1.0              # OLD - consider upgrading
langchain-openai==0.0.5       # OLD - major upgrade available
langgraph==0.0.20             # OLD - consider upgrading
openai==1.12.0
qdrant-client==1.7.0
sqlalchemy[asyncio]==2.0.23
pydantic==2.5.0
```

**Known Version Issues:**
- `langchain-openai==0.0.5` doesn't fully support `tool_choice` parameter
- Upgrading may break existing code but improve tool calling
- Test thoroughly after upgrades

---

## Project History & Phases

### Phase 1: Basic API (Completed)
- Product search, orders, context management
- Simple intent classification
- PostgreSQL database

### Phase 2: RAG Integration (Completed)
- Qdrant vector database
- Semantic search
- Knowledge base ingestion

### Phase 3: Multi-Agent System (Completed)
- LangGraph orchestration
- Supervisor + Sales + Support agents
- Memory Bank (customer facts)

### Phase 4: Evaluation & Hardening (In Progress)
- Golden dataset created
- LLM-as-a-judge evaluation
- Tool calling reliability issues

---

## Next Steps & Recommendations

### Immediate (Critical)
1. **Upgrade langchain-openai** to latest version
2. Test `tool_choice="required"` with new version
3. Implement fallback: if no tools called, manually invoke based on intent
4. Add retry mechanism at API level (not just in agents)

### Short Term
1. Implement request queuing for rate limiting
2. Add response caching for common queries
3. Optimize database queries (add indexes)
4. Implement proper logging aggregation (e.g., ELK stack)

### Long Term
1. Migrate to microservices if scaling needed
2. Implement A/B testing framework
3. Add user feedback loop
4. Build admin dashboard for monitoring
5. Implement multi-turn conversation optimization

---

## When NOT to Use This Architecture

**Don't use this if:**
- Need real-time latency <500ms (too many LLM calls)
- Budget is very tight (OpenAI costs add up)
- Simple rule-based logic would suffice
- No need for natural language understanding

**Better alternatives:**
- Rule-based chatbot (if <10 intents)
- Fine-tuned smaller model (if budget constrained)
- Retrieval-only system (if no actions needed)

---

## MCP (Model Context Protocol) Consideration

**Question**: Should we implement MCP?

**Answer**: **NO** - Not recommended for this architecture.

**Reasons:**
1. **In-Process Tools**: All tools are Python functions in same process
2. **Performance**: MCP adds serialization overhead (10-50ms per call)
3. **Complexity**: Additional networking layer not needed
4. **Native is Better**: LangChain native tool calling is faster and simpler

**When MCP IS useful:**
- Connecting to external services (Slack, GitHub)
- Cross-language tool invocation
- Microservices architecture
- Generic tool marketplaces

**Current setup is optimal for this use case.**

---

## Contact & Support

For issues with tool calling, check:
1. `app/agents/nodes.py` - Agent implementations
2. System prompts - Make them MORE imperative
3. LangChain version - Consider upgrading
4. OpenAI API logs - Check if tool_calls in response

For database issues:
1. Check connection in `database.py`
2. Verify credentials in `.env`
3. Check PostgreSQL logs

For Qdrant issues:
1. Warnings are harmless (already suppressed)
2. Check connection: `curl http://localhost:6333/collections`
3. Verify embeddings dimension matches

---

## License & Attribution

This is a bootcamp capstone project demonstrating:
- Multi-agent AI systems
- LangGraph orchestration
- Production FastAPI deployment
- RAG implementation
- Evaluation methodology

**Tech Stack**: FastAPI, LangChain, LangGraph, OpenAI, PostgreSQL, Qdrant

---

*Last Updated: 2025-11-27*
*Project Status: In Development (Tool calling issues being resolved)*
